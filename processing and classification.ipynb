{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "import csv\n",
    "import pandas as pd\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.stem.isri import ISRIStemmer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# لفائدتها في التصنيف لاحقاStopWords استثناء بعض الكلمات من ال "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Some StopWords Is Important SO Keep Them \n",
    "\n",
    "important=['ولا','لولا','نعم','ليس','لكن','كلا','ليست','لا']\n",
    "stopword=stopwords.words('arabic')\n",
    "stopword=[stopword.remove(K)for K in important if K in stopword]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bow Features And tfidf_features extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Bow Features And tfidf_features extraction\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "##############################################################################################\n",
    "def bow_features(X_train, X_test,ngram_range,minn,maxx):\n",
    "    count_vectorizer = CountVectorizer(lowercase =False,stop_words=stopword,\n",
    "                                       ngram_range=ngram_range,min_df=minn,max_df=maxx)\n",
    "    X_train_Vec =count_vectorizer.fit_transform(X_train) \n",
    "    X_test_Vec = count_vectorizer.transform(X_test) \n",
    "    \n",
    "    return X_train_Vec, X_test_Vec, count_vectorizer.vocabulary_, count_vectorizer\n",
    "##############################################################################################\n",
    "def tfidf_features(X_train, X_test,ngram_range,minn,maxx):\n",
    "    tfidf_vectorizer = TfidfVectorizer(lowercase=False,stop_words=stopword,\n",
    "                                       ngram_range=ngram_range,min_df=minn,max_df=maxx)\n",
    "    X_train =tfidf_vectorizer.fit_transform(X_train) \n",
    "    X_test = tfidf_vectorizer.transform(X_test) \n",
    "\n",
    "    return X_train, X_test, tfidf_vectorizer.vocabulary_, tfidf_vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training The Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training The Classifier Model\n",
    "\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "def train_classifier(X_train, y_train):\n",
    "    classifier = RidgeClassifier()\n",
    "    classifier.fit(X_train, y_train)\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  بحيث كل ملف يمثل عنصر منهاListقراءة مجلد داتا التدريب وتخزينه في "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Files 73\n"
     ]
    }
   ],
   "source": [
    "##Read Training Files From Folder And Store The Countent Of Them In List\n",
    "entries = os.listdir('Without_Processing\\\\Training_Data')\n",
    "\n",
    "data=[]\n",
    "all=0\n",
    "target=r'(?P<starter>\\|\\|\\s*)(?P<target>((POS|NEG|pos|neg)))'\n",
    "for entry in entries:\n",
    "    newText=[]\n",
    "    #print(entry)\n",
    "    f=open('Training_Data/'+(entry),'r',encoding='utf-8')\n",
    "    text=f.read()\n",
    "    text=text.split('\\n')\n",
    "    #text=[t.split('||')for t in text]\n",
    "    for t in text:\n",
    "        m = re.search(target,t)\n",
    "        if m:\n",
    "            clas=m.group('target')\n",
    "            temp=[re.sub(target,'',t),clas.upper()]\n",
    "            newText.append(temp) \n",
    "        \n",
    "    data.append(newText)  \n",
    "    f.close()\n",
    "    \n",
    "print(\"Number Of Files \"+str(len(data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# تصفية التويتات الغير مصنفة "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of All Tweets 7409\n"
     ]
    }
   ],
   "source": [
    "##now we Have List Of List Each List Represent Data OF File\n",
    "##Convert List Of List Of Tweets To List Of Tweets##\n",
    "file=0\n",
    "line=1\n",
    "all_data=[]\n",
    "all=0\n",
    "for text in data :\n",
    "    line=1\n",
    "    file+=1\n",
    "    for x in text :\n",
    "        all+=1\n",
    "        if len(x)!=2:\n",
    "            print(\"file %d\" %file)\n",
    "            #print(\" in line %d\" %line)    \n",
    "        elif x[1].upper().strip()==\"POS\" or x[1].upper().strip()==\"NEG\":\n",
    "            all_data.append([x[0],x[1].upper().strip()])\n",
    "            #print(\"OK\")\n",
    "        else:\n",
    "            print(\"Problem with file %d\" %file)\n",
    "            print(\" in line %d\" %line)    \n",
    "            print(x[0])\n",
    "        line+=1\n",
    "print(\"Number Of All Tweets \"+str(len(all_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  csvوتخزينها في ملفlistحذف التكرار من  ال"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6814\n",
      "Number Of Uniqe Tweets 6814\n"
     ]
    }
   ],
   "source": [
    "##Get Uniqe Tweets\n",
    "uniqe_data=[]\n",
    "[uniqe_data.append(x) for x in all_data if x not in uniqe_data]\n",
    "outfile=open('data.csv', 'w',encoding=\"utf-8\")\n",
    "writer = csv.writer(outfile)\n",
    "\n",
    "for t in uniqe_data:\n",
    "    writer.writerow(t)\n",
    "print(len(uniqe_data))\n",
    "for t in uniqe_data:\n",
    "    if(len(t)<2):\n",
    "        print (t)\n",
    "print(\"Number Of Uniqe Tweets \"+str(len(uniqe_data)))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# قراءة ملف التدريب csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainData = pd.read_csv('data.csv',names=['Tweet', 'Target'], encoding = 'utf-8')\n",
    "#TrainData.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POS    6023\n",
       "NEG    5278\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TrainData['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   بحيث كل ملف يمثل عنصر منهاListقراءة مجلد داتا الاختبار وتخزينه في "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of Files 37\n"
     ]
    }
   ],
   "source": [
    "##Read Testing Files From Folder And Store The Countent Of Them In List\n",
    "\n",
    "entries = os.listdir('Without_Processing\\\\Testing_Data')\n",
    "i=1\n",
    "data=[]\n",
    "target=r'(?P<starter>\\|\\|\\s*)(?P<target>((POS|NEG|pos|neg)))'\n",
    "newText=[]\n",
    "for entry in entries:\n",
    "    newText=[]\n",
    "    j=0\n",
    "    #print(entry)\n",
    "    f=open('Testing_Data/'+(entry),'r',encoding='utf-8')\n",
    "    text=f.read()\n",
    "    text=text.split('\\n')\n",
    "    for t in text:\n",
    "        j+=1\n",
    "        m = re.search(target,t)\n",
    "        if m:\n",
    "            clas=m.group('target')\n",
    "            temp=[re.sub(target,'',t),clas.upper()]\n",
    "            newText.append(temp) \n",
    "           \n",
    "    data.append(newText)\n",
    "    f.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "print(\"Number Of Files \"+str(len(data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  تصفية التويتات الغير مصنفة "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number Of All Tweets 7409\n"
     ]
    }
   ],
   "source": [
    "file=0\n",
    "line=1\n",
    "all_data=[]\n",
    "for text in data :\n",
    "    line=1\n",
    "    file+=1\n",
    "    for x in text :\n",
    "        if len(x)!=2:\n",
    "            print(\"file %d\" %file)\n",
    "            print(\" in line %d\" %line)    \n",
    "        elif x[1].upper().strip()==\"POS\" or x[1].upper().strip()==\"NEG\":\n",
    "            all_data.append([x[0],x[1].upper().strip()])\n",
    "            #print(\"OK\")\n",
    "        else:\n",
    "            print(\"Problem with file %d\" %file)\n",
    "            print(\" in line %d\" %line)    \n",
    "            print(x[0])\n",
    "        line+=1\n",
    "print(\"Number Of All Tweets \"+str(len(all_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# csvوتخزينها في ملفlistحذف التكرار من  ال"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6814\n",
      "Number Of Uniqe Tweets 6814\n"
     ]
    }
   ],
   "source": [
    "##Get uniqe Tweets\n",
    "uniqe_data=[]\n",
    "[uniqe_data.append(x) for x in all_data if x not in uniqe_data]\n",
    "outfile=open('Testdata.csv', 'w',encoding=\"utf-8\")\n",
    "writer = csv.writer(outfile)\n",
    "\n",
    "for t in uniqe_data:\n",
    "    writer.writerow(t)\n",
    "print(len(uniqe_data))\n",
    "for t in uniqe_data:\n",
    "    if(len(t)<2):\n",
    "        print (t)\n",
    "print(\"Number Of Uniqe Tweets \"+str(len(uniqe_data)))\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# قراءة ملف داتا الاختبار csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "Testdata = pd.read_csv('Testdata.csv',names=['Tweet', 'Target'], encoding = 'utf-8')\n",
    "#Testdata.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "POS    3435\n",
       "NEG    3379\n",
       "Name: Target, dtype: int64"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Testdata['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Text_Cleaning_and_Normalization\n",
    "حذف الروابط والصور والحروف المكررة وعلامات الترقيم\n",
    "وتبديل بعض الحروف كالهمزة و... بأحرف بديلة \n",
    "حذف المنشن والريتويت Mention And Retweet\n",
    "حذف علامات التشكيل\n",
    "\n",
    "#     Text_Analysis\n",
    "Steemingارجاع الكلمات للجذر\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(text):\n",
    "    text = re.sub(\"ة\", \"ه\",text)\n",
    "    text = re.sub(\"[إأآا]\", \"ا\",text)\n",
    "    text = re.sub(\"ى\", \"ي\",text)\n",
    "    text = re.sub(\"ئ\", \"ء\",text)\n",
    "    text=re.sub(r'(.)\\1+', r'\\1\\1',text)\n",
    "    return text\n",
    "\n",
    "###############################################################################################\n",
    "def Text_Cleaning_and_Normalization(text):\n",
    "    URLPic=r'pic.twitter.com/[A-Za-z0-9]+'#الصور\n",
    "    URLLink=r'http(s)?://(\\s)*[A-Za-z]+\\.[A-Za-z]+((/)?([A-Za-z0-9])+(\\s)?)+'#الروابط\n",
    "    punctuations_Symbols = r'[\\.؟!;()&,%\"''?]'#علامات الترقيم\n",
    "    arabic_diacritics = r' َ  ُ ِ ّ  ً  ٌ ٍ  ْ '#علامات التشكيل\n",
    "    EngNames=r'@[A-Za-z0-9_ا-ي]+'#منشن او ريتويت باللغة الانكليزية\n",
    "    ArbNames=r'[0-9_ا-ي]+@'#منشن اة ريتويت باللغة العربية\n",
    "\n",
    "    \n",
    "    text=re.sub(URLLink,'',text)\n",
    "    text=re.sub(URLPic,'',text)\n",
    "    text=re.sub(punctuations_Symbols,'',text)\n",
    "    text=re.sub(arabic_diacritics,'',text)\n",
    "    text=re.sub(EngNames,'',text)\n",
    "    text=re.sub(ArbNames,'',text)\n",
    "    text=normalize(text)\n",
    "    return text\n",
    "\n",
    "###############################################################################################\n",
    "#Text_Analysis\n",
    "stop_words = set(stopwords.words('arabic'))\n",
    "def Text_Analysis(text,isstem):\n",
    "    if isstem:\n",
    "        text=text.split()\n",
    "        st = ISRIStemmer()\n",
    "        text=[st.stem(w)for w in text]\n",
    "        text=''.join(text)\n",
    "    return text\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# استبدال  الهاشتاغات\n",
    "# إزالة الهاشتاغات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_hashtag(text):\n",
    "    hashtag=r'#([\\w]|[0-9])+'\n",
    "    text=re.sub(hashtag,'',text)\n",
    "    return text\n",
    "##############################################################################################\n",
    "def convert_hashtag(text):\n",
    "    \n",
    "    #print(text)\n",
    "    hashtag=r'#|_'\n",
    "    text=re.sub(hashtag,' ',text)\n",
    "\n",
    "    #print(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# اختبار استبدال وازالة الهاشتاغ على مثال من التويتات"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example For Removing and Converting Hashtags\n",
      "remove_hashtag \n",
      " يجب أن تسقط الولاية، رغم أن سقوطها وحده لا يكفي يحب أن يوفر الدعم للمرأة في التعليم و العمل و تكوين أسرة \n",
      "convert_hashtag \n",
      " سعوديات نطلب اسقاط الولايه692 يجب أن تسقط الولاية، رغم أن سقوطها وحده لا يكفي يحب أن يوفر الدعم للمرأة في التعليم و العمل و تكوين أسرة \n"
     ]
    }
   ],
   "source": [
    "print(\"Example For Removing and Converting Hashtags\")\n",
    "te='#سعوديات_نطلب_اسقاط_الولايه692 يجب أن تسقط الولاية، رغم أن سقوطها وحده لا يكفي يحب أن يوفر الدعم للمرأة في التعليم و العمل و تكوين أسرة '\n",
    "print(\"remove_hashtag \\n\"+remove_hashtag(str(te)))\n",
    "te='#سعوديات_نطلب_اسقاط_الولايه692 يجب أن تسقط الولاية، رغم أن سقوطها وحده لا يكفي يحب أن يوفر الدعم للمرأة في التعليم و العمل و تكوين أسرة '\n",
    "print(\"convert_hashtag \\n\"+convert_hashtag(str(te)) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#(tweet,Target\\pos|neg)تجهيز الداتا بحيث كل عنصر يمثل ك"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata = [tuple(x) for x in TrainData.values]\n",
    "testdata = [tuple(x) for x in Testdata.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  للكلماتsteeming تابع يقوم بتنظيف الداتا وعمل "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Cleaning And Steeming\n",
    "def preprocess(traindata,testdata,isstem,isclean):\n",
    "    if isclean:\n",
    "        train_data_norm=[(Text_Cleaning_and_Normalization(str(t[0])),t[1])for t in traindata]\n",
    "        test_data_norm=[(Text_Cleaning_and_Normalization(str(t[0])),t[1])for t in testdata]\n",
    "    else:\n",
    "        train_data_norm=traindata\n",
    "        test_data_norm=testdata\n",
    "\n",
    "    train_data_norm=[(Text_Analysis(str(t[0]),isstem),t[1])for t in train_data_norm] \n",
    "    test_data_norm=[(Text_Analysis(str(t[0]),isstem),t[1])for t in test_data_norm]\n",
    "    return train_data_norm,test_data_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# X وYفصل الداتا ل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splite Data to Training And Testing\n",
    "def splite_data(traindata,testdata,isstem,isclean):\n",
    "    train_data_norm,test_data_norm=preprocess(traindata,testdata,isstem,isclean)\n",
    "    training_set_size = int(len(train_data_norm))\n",
    "    testing_set_size = int(len(test_data_norm))\n",
    "\n",
    "    X_train = [example[0] for example in train_data_norm]\n",
    "    y_train = [example[1] for example in train_data_norm]\n",
    "\n",
    "    X_test = [example[0] for example in test_data_norm]\n",
    "    y_test = [example[1] for example in test_data_norm]\n",
    "    return X_train,y_train,X_test,y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# تابع يقوم ب\n",
    "## Get Features Of Data With bow And tfidf\n",
    "## Then Training And Testing The Model \n",
    "## ComputeThe Accuracy\n",
    "#### min_df , max_dfو البارامترين N_Gramنقوم بتمرير داتا التدريب وداتا الاختبار وعدد ال \n",
    "####  وتدريب المودل features نقوم بإيجاد ال\n",
    "#### accuracyنقوم باختبار المودل وحساب وطباعة ال "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Get Features Of Data With bow And tfidf\n",
    "##Then Training And Testing The Model \n",
    "## ComputeThe Accuracy \n",
    "def Get_Result(X_train,X_test,y_train,y_test,ngrams,minn,maxx):\n",
    "    X_train_bag, X_test_bag, bow_vocab, bow_vec= bow_features(X_train, X_test,ngrams,minn,maxx)\n",
    "    X_train_tfidf, X_test_tfidf, tfidf_vocab, tfidf_vec= tfidf_features(X_train, X_test,ngrams,minn,maxx)\n",
    "    #############################################################################################\n",
    "    classifier_bag = train_classifier(X_train_bag, y_train)\n",
    "    classifier_tfidf = train_classifier(X_train_tfidf, y_train)\n",
    "    #############################################################################################\n",
    "    y_test_predicted_labels_bag = classifier_bag.predict(X_test_bag)\n",
    "    y_test_predicted_scores_bag = classifier_bag.decision_function(X_test_bag)\n",
    "\n",
    "    y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "    y_test_predicted_scores_tfidf = classifier_tfidf.decision_function(X_test_tfidf)\n",
    "    #############################################################################################\n",
    "    #UniGramm Without Steeming\n",
    "    from sklearn.metrics import accuracy_score\n",
    "\n",
    "    print('Bag-of-words Accuracy: '+ str(accuracy_score(y_test, y_test_predicted_labels_bag)))\n",
    "    print('Tfidf Accuracy: ' + str(accuracy_score(y_test, y_test_predicted_labels_tfidf)))\n",
    "    \n",
    "    return bow_vec,tfidf_vec,classifier_bag,classifier_tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) تجريب المودل  Bigram##Without Cleaning The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,Y_train,X_test,Y_test=splite_data(traindata,testdata,False,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Bigram##Without Cleaning The Data \n",
      "Bag-of-words Accuracy: 0.8095098326973877\n",
      "Tfidf Accuracy: 0.8219841502788376\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "print(\"Bigram##Without Cleaning The Data \")\n",
    "bow_vec,tfidf_vec,classifier_bag,classifier_tfidf=Get_Result(X_train,X_test,Y_train,Y_test,(1,2),1,0.09)\n",
    "print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) تجريب المودل  Unigram##Without Cleaning The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Unigram##Without Cleaning The Data \n",
      "Bag-of-words Accuracy: 0.7983563252127972\n",
      "Tfidf Accuracy: 0.8168476665688289\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "print(\"Unigram##Without Cleaning The Data \")\n",
    "bow_vec,tfidf_vec,classifier_bag,classifier_tfidf=Get_Result(X_train,X_test,Y_train,Y_test,(1,1),1,0.09)\n",
    "print('------------------------------------------------------')\n",
    "#Bigram Give Higher Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) تجريب المودل Bigram#With Cleaning The Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "#Bigram#With Cleaning The Data \n",
      "Bag-of-words Accuracy: 0.8128852362782507\n",
      "Tfidf Accuracy: 0.82169063692398\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "X_train,Y_train,X_test,Y_test=splite_data(traindata,testdata,False,True)\n",
    "print(\"#Bigram#With Cleaning The Data \")\n",
    "bow_vec,tfidf_vec,classifier_bag,classifier_tfidf=Get_Result(X_train,X_test,Y_train,Y_test,(1,2),1,0.09)\n",
    "print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) تجريب المودلBigram##With Word Steeming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Bi##With Word Steeming\n",
      "Bag-of-words Accuracy: 0.7001761080129146\n",
      "Tfidf Accuracy: 0.6916642207220428\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "X_train,Y_train,X_test,Y_test=splite_data(traindata,testdata,True,True)\n",
    "print(\"Bi##With Word Steeming\")\n",
    "bow_vec,tfidf_vec,classifier_bag,classifier_tfidf=Get_Result(X_train,X_test,Y_train,Y_test,(1,2),1,0.09)\n",
    "print('----------------------------------------------------------------')\n",
    "##Steeming Word Give Less Accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5/Removing Hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('#سعوديات_نطلب_اسقاط_الولايه692 يجب أن تسقط الولاية، رغم أن سقوطها وحده لا يكفي يحب أن يوفر الدعم للمرأة في التعليم و العمل و تكوين أسرة ', 'POS')\n",
      "(' يجب أن تسقط الولاية، رغم أن سقوطها وحده لا يكفي يحب أن يوفر الدعم للمرأة في التعليم و العمل و تكوين أسرة ', 'POS')\n"
     ]
    }
   ],
   "source": [
    "###Removing Hashtags fromData\n",
    "traindata_Copy=traindata.copy()\n",
    "testdata_Copy=testdata.copy()\n",
    "print(traindata_Copy[0])\n",
    "traindata_Copy=[(remove_hashtag(str(t[0])),t[1])for t in traindata_Copy]\n",
    "testdata_Copy=[(remove_hashtag(str(t[0])),t[1])for t in testdata_Copy]\n",
    "print(traindata_Copy[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) With Removing Hashtags & Without Cleaning The Data  تجريب المودل "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "With Removing Hashtags & Without Cleaning The Data  \n",
      "Bag-of-words Accuracy: 0.6721455826240094\n",
      "Tfidf Accuracy: 0.6725858526562959\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "X_train,Y_train,X_test,Y_test=splite_data(traindata_Copy,testdata_Copy,True,True)\n",
    "print(\"With Removing Hashtags & Without Cleaning The Data  \")\n",
    "bow_vec,tfidf_vec,classifier_bag,classifier_tfidf=Get_Result(X_train,X_test,Y_train,Y_test,(1,2),1,0.09)\n",
    "print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Bigram##With Removing Hashtags & With Cleaning The Dataتجريب المودل "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Bigram##With Removing Hashtags & With Cleaning The Data \n",
      "Bag-of-words Accuracy: 0.8005576753742295\n",
      "Tfidf Accuracy: 0.81171118285882\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "X_train,Y_train,X_test,Y_test=splite_data(traindata_Copy,testdata_Copy,False,True)\n",
    "print(\"Bigram##With Removing Hashtags & With Cleaning The Data \")\n",
    "bow_vec,tfidf_vec,classifier_bag,classifier_tfidf=Get_Result(X_train,X_test,Y_train,Y_test,(1,2),1,0.09)\n",
    "print('------------------------------------------------------')\n",
    "\n",
    "##Model With HAshtags Give Higher Accuracy Than Removing Hashtags "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7)  With Removing Hashtags & With Word Steemingتجريب المودل"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "With Removing Hashtags & With Word Steeming\n",
      "Bag-of-words Accuracy: 0.6721455826240094\n",
      "Tfidf Accuracy: 0.6725858526562959\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "X_train,Y_train,X_test,Y_test=splite_data(traindata_Copy,testdata_Copy,True,True)\n",
    "print(\"With Removing Hashtags & With Word Steeming\")\n",
    "bow_vec,tfidf_vec,classifier_bag,classifier_tfidf=Get_Result(X_train,X_test,Y_train,Y_test,(1,2),1,0.09\n",
    "                                                            \n",
    "                                                            )\n",
    "print('------------------------------------------------------')\n",
    "#Steeming Word Give Bad Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6\\Converting Hashtags To Separate  Words \n",
    "# تحويل الهاشتغات الى كلمات منفصلة"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mExample Before Converting\u001b[0m\n",
      "('#سعوديات_نطلب_اسقاط_الولايه692 يجب أن تسقط الولاية، رغم أن سقوطها وحده لا يكفي يحب أن يوفر الدعم للمرأة في التعليم و العمل و تكوين أسرة ', 'POS')\n",
      "\u001b[32mExample After Converting\u001b[0m\n",
      "(' سعوديات نطلب اسقاط الولايه692 يجب أن تسقط الولاية، رغم أن سقوطها وحده لا يكفي يحب أن يوفر الدعم للمرأة في التعليم و العمل و تكوين أسرة ', 'POS')\n"
     ]
    }
   ],
   "source": [
    "###Converting Hashtags\n",
    "traindata_Copy=traindata.copy()\n",
    "testdata_Copy=testdata.copy()\n",
    "print(colored(\"Example Before Converting\",'green'))\n",
    "print(traindata_Copy[0])\n",
    "traindata=[(convert_hashtag(str(t[0])),t[1])for t in traindata_Copy]\n",
    "testdata=[(convert_hashtag(str(t[0])),t[1])for t in testdata_Copy]\n",
    "print(colored(\"Example After Converting\",'green'))\n",
    "print(traindata[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7)  تجريب المودل \n",
    "## Bigarm##With Convering Hashtags & Without Cleaning The Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Bigarm##With Convering Hashtags & Without Cleaning The Data  \n",
      "Bag-of-words Accuracy: 0.7001761080129146\n",
      "Tfidf Accuracy: 0.691517464044614\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "X_train,Y_train,X_test,Y_test=splite_data(traindata_Copy,testdata_Copy,True,True)\n",
    "print(\"Bigarm##With Convering Hashtags & Without Cleaning The Data  \")\n",
    "Get_Result(X_train,X_test,Y_train,Y_test,(1,2),1,0.09)\n",
    "print('------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Best Accuracy\n",
    "##  بعد تحويل الهاشتاغات وتنظيف الداتا من علامات الترقيم والاحرف المكررة والروابط والصور وحذف المنشن والريتويت \n",
    "## Bigramباستخدام "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mThe Best Accuracy\u001b[0m\n",
      "With Cleaning Data , Converting Hashtags and Using Bigram\n",
      "------------------------------------------------------\n",
      "Bigram##With Converting Hashtags & With Cleaning The Data \n",
      "Bag-of-words Accuracy: 0.8130319929556795\n",
      "Tfidf Accuracy: 0.8218373936014088\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(colored(\"The Best Accuracy\",'blue'))\n",
    "print(\"With Cleaning Data , Converting Hashtags and Using Bigram\")\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "X_train,Y_train,X_test,Y_test=splite_data(traindata_Copy,testdata_Copy,False,True)\n",
    "print(\"Bigram##With Converting Hashtags & With Cleaning The Data \")\n",
    "Get_Result(X_train,X_test,Y_train,Y_test,(1,2),1,0.09)\n",
    "print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## تدريب المودل Unigram##With Converting Hashtags & With Cleaning The Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Unigram##With Converting Hashtags & With Cleaning The Data \n",
      "Bag-of-words Accuracy: 0.8008511887290872\n",
      "Tfidf Accuracy: 0.8156736131493983\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "X_train,Y_train,X_test,Y_test=splite_data(traindata_Copy,testdata_Copy,False,True)\n",
    "print(\"Unigram##With Converting Hashtags & With Cleaning The Data \")\n",
    "Get_Result(X_train,X_test,Y_train,Y_test,(1,1),1,0.09)\n",
    "print('------------------------------------------------------')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Best Accuracy\n",
    "##  بعد تحويل الهاشتاغات وتنظيف الداتا من علامات الترقيم والاحرف المكررة والروابط والصور وحذف المنشن والريتويت \n",
    "## treegramباستخدام  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Treegram##With Converting Hashtags & With Cleaning The Data \n",
      "Bag-of-words Accuracy: 0.8140592896976813\n",
      "Tfidf Accuracy: 0.8230114470208394\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "X_train,Y_train,X_test,Y_test=splite_data(traindata_Copy,testdata_Copy,False,True)\n",
    "print(\"Treegram##With Converting Hashtags & With Cleaning The Data \")\n",
    "Get_Result(X_train,X_test,Y_train,Y_test,(1,3),1,0.09)\n",
    "print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## تجريب المودل\n",
    "## Bigram##With Converting Hashtags & With Word Steeming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Bi##With Converting Hashtags & With Word Steeming\n",
      "Bag-of-words Accuracy: 0.6758144995597299\n",
      "Tfidf Accuracy: 0.6758144995597299\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('------------------------------------------------------')\n",
    "X_train,Y_train,X_test,Y_test=splite_data(traindata_Copy,testdata_Copy,True,True)\n",
    "print(\"Bi##With Converting Hashtags & With Word Steeming\")\n",
    "Get_Result(X_train,X_test,Y_train,Y_test,(1,2),1,0.09)\n",
    "print('------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3\\ Get Input From User "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "مع اسقاط الولاية\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'POS'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet=input()\n",
    "tweet=Text_Cleaning_and_Normalization(tweet)\n",
    "tweetfiture=tfidf_vec.transform([tweet])\n",
    "classifier_tfidf.predict(tweetfiture)[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## تحميل داتا اضافية محايدة لعمل Multi Classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import GetOldTweets3 \n",
    "import got3 as got"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Download Data And Save It In File \n",
    "tweetCriteria = GetOldTweets3.manager.TweetCriteria()\\\n",
    "                                     .setQuerySearch('مباراة')\\\n",
    "                                     .setNear(\"Saudi Arabia\")\\\n",
    "                                     .setSince(\"2017-11-16\")\\\n",
    "                                     .setUntil(\"2019-11-18\")\\\n",
    "                                     .setMaxTweets(1000)\\\n",
    "\n",
    "tweet = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "file = open(\"Tweets.txt\",'w',encoding='utf-8') \n",
    "for t in tweet:\n",
    "    file.write(t.text+ \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (tweet,target\\'nothing')قراءة الداتا الإضافية من الملف وتصنيفها ك\n",
    "### لإعادة تقسيمها بشكل متوازنlist دمج الداتا في \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Tweets.txt','r',encoding='utf-8')\n",
    "text=f.read()\n",
    "text=text.split('\\n')\n",
    "additional_data=[(t,'Nothing')for t in text]\n",
    "[traindata.append(t)for t in testdata if t not in traindata]\n",
    "[traindata.append(t)for t in additional_data]\n",
    "\n",
    "print(len(traindata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### تقسيم الداتا بشكل متوازن لداتا تدريب وداتا اختبار "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11582\n",
      "11582\n",
      "4964\n",
      "4964\n"
     ]
    }
   ],
   "source": [
    "training_set_size = int(len(traindata)*0.70)\n",
    "X = [example[0] for example in traindata]\n",
    "y = [example[1] for example in traindata]\n",
    "\n",
    "X_train = X[:training_set_size]\n",
    "y_train = y[:training_set_size]\n",
    "\n",
    "X_test = X[training_set_size:]\n",
    "y_test = y[training_set_size:]\n",
    "\n",
    "print(len(X_train))\n",
    "print(len(y_train))\n",
    "print(len(X_test))\n",
    "print(len(y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### إعادة تدريب المودل كمسألة Multi Class Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vec,tfidf_vec,classifier_bag,classifier_tfidf=Get_Result(X_train,X_test,Y_train,Y_test,(1,2),1,0.09)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
